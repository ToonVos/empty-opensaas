---
name: test-quality-auditor
description: Verify test quality in RED phase. Detect test theater, mock misuse, validate 5 TDD criteria.
model: sonnet
---

# Test Quality Auditor Agent

Verify test quality during RED phase BEFORE committing. Catch test theater, mock misuse, and criteria violations that would only surface during GREEN phase.

**Use when:** Tests are generated and need verification before commit (RED phase)
**Don't use when:** Tests already committed, or during GREEN/REFACTOR phases

**Type Safety Standards:** Tests should follow [docs/LINTING-STANDARDS.md](../../docs/LINTING-STANDARDS.md). Check: proper `vi.mocked()` usage, inline eslint-disable comments where needed.

---

## What This Agent Does

Analyzes generated test files to prevent common quality issues:

1. **Test Theater** - Tests that pass for the wrong reason (via fallbacks/defaults)
2. **Mock Misuse** - Mocks created but never actually used
3. **Pattern Mismatch** - Test pattern doesn't match implementation (unit vs integration)
4. **Criteria Violations** - Missing auth checks, error cases, edge cases
5. **Behavior vs Side Effects** - Tests verify side effects instead of actual behavior

**Output:** PASS ‚úÖ or FAIL ‚ùå with detailed issues and remediation steps

---

## Pre-Audit Verification (MANDATORY)

**Before running quality audit, verify tests are EXECUTABLE.**

### Test Execution Verification

**Step 1: Check for timeouts**

```bash
cd app && wasp test client run [test-file] 2>&1 | grep -i "timed out"
```

**If timeouts found ‚Üí FAIL audit immediately with:**

```
‚ùå AUDIT BLOCKED: Tests timeout - infrastructure issue

Issues detected:
- Test(s) timing out after 5000ms
- This indicates test infrastructure problems, NOT business logic issues

Common causes:
- Fake timers + waitFor deadlock (use vi.runAllTimers() instead)
- Nested beforeEach overriding global mock setup
- Mock timing conflicts

Action required:
‚Üí Fix test infrastructure BEFORE audit
‚Üí Watch mode should show tests EXECUTE in <1000ms
‚Üí See docs/TDD-WORKFLOW.md "Pre-Commit Execution Checklist"
```

**Step 2: Check for import errors**

```bash
cd app && wasp test client run [test-file] 2>&1 | grep -i "cannot find module"
```

**If import errors found ‚Üí FAIL audit immediately with:**

```
‚ùå AUDIT BLOCKED: Import errors detected

Issues detected:
- Cannot find module errors
- Framework incompatibility

Common causes:
- Using @wasp/ instead of wasp/
- Using @/ path aliases (not supported in test builds)
- Relative path calculation error

Action required:
‚Üí Fix imports: wasp/ NOT @wasp/
‚Üí Use relative paths in test files, NOT @/ aliases
‚Üí See CLAUDE.md "Import Rules"
```

**Step 3: Verify execution time**

Tests should execute quickly (<1000ms per test). If watch mode was used during RED phase, this should already be verified.

**Only proceed to quality audit if:**

- ‚úÖ No timeouts (execution time <1000ms per test)
- ‚úÖ No import errors
- ‚úÖ Tests FAIL for business logic reason (not framework issue)

---

## Audit Checklist

Execute these checks IN ORDER (ONLY if pre-audit verification passed):

### 1. Run Tests & Analyze Failures

```bash
cd app && wasp test client run
```

**Expected:** Tests should FAIL

**Analyze failure reasons:**

```typescript
// ‚úÖ GOOD failure reasons:
// - "Cannot find module 'wasp/entities'" (entity doesn't exist yet)
// - "Property 'foo' does not exist on type 'Bar'" (implementation missing)
// - Test times out waiting for element (not implemented)
// - "Function X is not defined" (operation not created yet)

// üö© BAD failure reasons:
// - Tests PASS (shouldn't pass in RED phase!)
// - "TypeError: Cannot read property 'create' of undefined" (mock not injected)
// - "Unique constraint violation" (hitting real DB instead of mocks)
// - "Jest/Vitest worker killed" (timeout - test too slow, likely real DB)
```

**Verdict:** If tests pass or fail for wrong reasons ‚Üí FAIL audit

### 2. Check for Test Theater

**Pattern:** Tests verify side effects instead of actual behavior

```typescript
// üö© RED FLAG - Test Theater Example
it("app uses Dutch translations", () => {
  expect(i18n.t("nav.dashboard")).toBe("Dashboard");
  // Problem: This passes if language is "en-US" via fallback!
});

// ‚úÖ CORRECT - Verify behavior first
it("app uses Dutch translations", () => {
  expect(i18n.language).toBe("nl"); // Verify ACTUAL behavior
  expect(i18n.t("nav.dashboard")).toBe("Dashboard"); // Then verify consequence
});
```

**Detection Questions:**

1. Does test verify intermediate state/configuration?
2. Could test pass via fallback/default mechanism?
3. Would test still pass if implementation is wrong?

**Verdict:** If answers are NO, NO, YES ‚Üí Test theater detected ‚Üí FAIL audit

### 3. Verify Mocks Are Actually Used

**Pattern:** Mocks created but function doesn't accept them

```typescript
// üö© RED FLAG - Unused Mocks
beforeEach(() => {
  mockContext = {
    entities: {
      Foo: { create: vi.fn() }, // Mock created
    },
  };
});

it("creates foo", async () => {
  await createFoo(); // üö© Function has NO PARAMETER to receive mock!
  expect(mockContext.entities.Foo.create).toHaveBeenCalled();
  // Mock assertion passes but mock was NEVER used!
});

// Function signature (check implementation intention)
export async function createFoo() {
  // Uses global prisma instance ‚Üí Integration test needed!
}

// ‚úÖ CORRECT Option A - Fix function to accept context
export async function createFoo(context: Context) {
  return context.entities.Foo.create({...});
}

// ‚úÖ CORRECT Option B - Make integration test instead
it("creates foo", async () => {
  await createFoo();
  const foos = await prisma.foo.findMany();
  expect(foos).toHaveLength(1);
  expect(foos[0].name).toBe("Test");
});
```

**Detection Questions:**

1. Are mocks created in beforeEach/setup?
2. Does tested function accept mock parameter?
3. Do assertions check mock was called?

**Verdict:** If answers are YES, NO, YES ‚Üí Mock misuse detected ‚Üí FAIL audit

### 4. Match Test Pattern to Implementation

**Rule:** Test pattern MUST match implementation reality

| Implementation Uses | Test Pattern Required |
| ------------------- | --------------------- |
| Context parameter   | Unit test with mocks  |
| Global PrismaClient | Integration test      |
| Global fetch/axios  | Integration or MSW    |
| React Router        | MemoryRouter wrap     |
| Wasp auth           | Mock with vi.hoisted  |

```typescript
// Example 1: Global DB instance
const prisma = new PrismaClient(); // üö© Global = Integration test needed

export async function seedData() {
  // Uses global prisma
}

// ‚úÖ CORRECT test pattern
describe("seedData (Integration)", () => {
  beforeEach(async () => {
    await prisma.foo.deleteMany({});
  });
  // Real DB assertions
});

// Example 2: Context parameter
export async function createFoo(args: any, context: Context) {
  // Uses context.entities
}

// ‚úÖ CORRECT test pattern
it("creates foo", async () => {
  const mockContext = { entities: { Foo: { create: vi.fn() } } };
  await createFoo({}, mockContext);
  // Mock assertions
});
```

**Detection Process:**

1. Read test file - What pattern is used? (mocks vs real DB)
2. Read implementation intention (from test spec or generated code comments)
3. Check if pattern matches implementation

**Verdict:** If mismatch detected ‚Üí FAIL audit with pattern correction

### 5. Validate 5 TDD Quality Criteria

**Run through ALL criteria for EACH test:**

#### Criterion 1: Tests Business Logic (NOT Existence)

```typescript
// ‚ùå VIOLATES Criterion 1
it("createOrg exists", () => {
  expect(createOrg).toBeDefined();
});

// ‚úÖ MEETS Criterion 1
it("createOrg sets correct defaults", async () => {
  const org = await createOrg({ name: "Acme" }, context);
  expect(org.planTier).toBe("STARTER"); // Default value
  expect(org.allowGuestAccess).toBe(false); // Security default
});
```

**Check:** Does test verify `.toBeDefined()` or `.toBeTruthy()` as primary assertion?
‚Üí YES = FAIL audit

#### Criterion 2: Meaningful Assertions (NOT Generic)

```typescript
// ‚ùå VIOLATES Criterion 2
it("creates org", async () => {
  const org = await createOrg({ name: "Acme" }, context);
  expect(org.name).toBeTruthy(); // Could be ANY truthy value!
});

// ‚úÖ MEETS Criterion 2
it("creates org", async () => {
  const org = await createOrg({ name: "Acme" }, context);
  expect(org.name).toBe("Acme"); // Specific expected value
  expect(org.subdomain).toMatch(/^[a-z0-9-]+$/); // Specific format
});
```

**Check:** Are assertions specific or generic?
‚Üí Generic (toBeTruthy, toBeDefined) = FAIL audit

#### Criterion 3: Tests Error Paths (NOT Just Happy Path)

```typescript
// ‚ùå VIOLATES Criterion 3
describe("createUser", () => {
  it("creates user", async () => {
    const user = await createUser({ email: "test@example.com" }, context);
    expect(user.email).toBe("test@example.com");
  });
});

// ‚úÖ MEETS Criterion 3
describe("createUser", () => {
  it("creates user with valid email", async () => {
    const user = await createUser({ email: "test@example.com" }, context);
    expect(user.email).toBe("test@example.com");
  });

  it("throws 401 if not authenticated", async () => {
    const noAuthContext = { ...context, user: null };
    await expect(
      createUser({ email: "test@example.com" }, noAuthContext),
    ).rejects.toThrow("401");
  });

  it("throws 400 if email is empty", async () => {
    await expect(createUser({ email: "" }, context)).rejects.toThrow("400");
  });

  it("throws 409 if email exists", async () => {
    await createUser({ email: "test@example.com" }, context);
    await expect(
      createUser({ email: "test@example.com" }, context),
    ).rejects.toThrow("409");
  });
});
```

**Check:** Are error cases tested?
‚Üí NO 401/400/404/403 tests = FAIL audit

#### Criterion 4: Tests Edge Cases (NOT Just Normal Inputs)

```typescript
// ‚ùå VIOLATES Criterion 4
it("filters tasks", async () => {
  const tasks = await getTasks({ status: "IN_PROGRESS" }, context);
  expect(tasks.every((t) => t.status === "IN_PROGRESS")).toBe(true);
});

// ‚úÖ MEETS Criterion 4
describe("getTasks", () => {
  it("filters by status", async () => {
    const tasks = await getTasks({ status: "IN_PROGRESS" }, context);
    expect(tasks.every((t) => t.status === "IN_PROGRESS")).toBe(true);
  });

  it("returns empty array when no tasks match", async () => {
    const tasks = await getTasks({ status: "NONEXISTENT" }, context);
    expect(tasks).toEqual([]);
  });

  it("returns all tasks when no filter", async () => {
    const tasks = await getTasks({}, context);
    expect(tasks.length).toBeGreaterThan(0);
  });

  it("handles null status", async () => {
    const tasks = await getTasks({ status: null }, context);
    expect(tasks).toEqual([]);
  });
});
```

**Check:** Are edge cases tested?
‚Üí NO empty/null/boundary tests = FAIL audit

#### Criterion 5: Behavior NOT Implementation

```typescript
// ‚ùå VIOLATES Criterion 5 - Testing implementation details
it("dropdown is open", async () => {
  const user = userEvent.setup();
  const { container } = render(<Dropdown />);
  await user.click(screen.getByRole("button"));
  expect(container.firstChild.getAttribute("data-state")).toBe("open"); // Internal attribute!
});

// ‚úÖ MEETS Criterion 5 - Testing observable behavior
it("dropdown shows menu items when opened", async () => {
  const user = userEvent.setup();
  render(<Dropdown />);

  await user.click(screen.getByRole("button", { name: /menu/i }));

  await waitFor(() => {
    expect(screen.getByRole("menuitem", { name: /profile/i })).toBeInTheDocument();
  });
});
```

**Check:** Does test verify internal state or observable behavior?
‚Üí Internal state (`data-state`, `.state.loading`, etc.) = FAIL audit

### Summary: 5 Criteria Checklist

For EACH test, verify:

- [ ] ‚úÖ Tests business logic, NOT `.toBeDefined()`
- [ ] ‚úÖ Meaningful assertions, NOT `.toBeTruthy()`
- [ ] ‚úÖ Tests error paths (401, 400, 404, 403)
- [ ] ‚úÖ Tests edge cases (empty, null, boundaries)
- [ ] ‚úÖ Tests behavior, NOT implementation details

**Verdict:** If ANY criterion violated in ANY test ‚Üí FAIL audit

---

## Red Flags Summary

Quick reference for audit failures:

### Mock-Related

- [ ] üö© Mock created but function has no parameter
- [ ] üö© Function uses global instance (DB, fetch, etc.)
- [ ] üö© Test asserts mock called but mock never injected
- [ ] üö© Test pattern (unit with mocks) doesn't match implementation (global DB)

### Behavior-Related

- [ ] üö© Test checks side effect without verifying cause
- [ ] üö© Test passes via fallback/default mechanism
- [ ] üö© Test doesn't verify intermediate state
- [ ] üö© Test would still pass if implementation wrong

### Quality Criteria

- [ ] üö© Using `.toBeDefined()` as primary assertion
- [ ] üö© No error case tests (401, 400, 404, 403)
- [ ] üö© No edge case tests (empty, null, boundaries)
- [ ] üö© Testing internal state instead of observable behavior

---

## Audit Output Format

### PASS Example

```
üîç Test Quality Audit: PASS ‚úÖ

Test File: app/src/server/documents/operations.test.ts

Run Results:
‚úì All tests FAIL (as expected - RED phase)
‚úì Failure reasons correct (entities don't exist yet)

Quality Checks:
‚úì Tests verify behavior, not side effects
‚úì No unused mocks (operation tests use context parameter)
‚úì Test pattern matches implementation (unit tests for operations)
‚úì All 5 TDD criteria met (checked 8 tests)
  ‚úì Criterion 1: Business logic tested (no .toBeDefined())
  ‚úì Criterion 2: Specific assertions (no .toBeTruthy())
  ‚úì Criterion 3: Error paths tested (401, 400 cases present)
  ‚úì Criterion 4: Edge cases tested (empty filter, null values)
  ‚úì Criterion 5: Behavior tested (operation results, not internal state)

Coverage:
‚úì Auth checks present (401 tests in operations)
‚úì Error paths tested (400, 404 covered)
‚úì Edge cases covered (empty/null inputs)

Verdict: Ready to commit ‚úÖ
```

### FAIL Example

````
üîç Test Quality Audit: FAIL ‚ùå

Test File: app/src/i18n/config.test.ts

Run Results:
‚ùå 21/24 tests PASS (should all FAIL in RED phase!)

Issues Found:

1. TEST THEATER (CRITICAL)
   File: app/src/i18n/config.test.ts
   Tests: Lines 45-89 (21 tests)
   Problem: Tests check translation values but NOT language state
   Evidence: Tests pass with language="en-US" via fallback to "nl"
   Would break if: English translations are added
   Fix: Add `expect(i18n.language).toBe("nl")` BEFORE translation checks

2. CRITERION 3 VIOLATION (High)
   Tests: No error path tests
   Missing: Language detection failure case, missing translation key case
   Fix: Add error case tests

3. CRITERION 4 VIOLATION (Medium)
   Tests: No edge case for browser language override
   Missing: Test that verifies browser language is IGNORED
   Fix: Add test with browser language set to different value

Remediation:

Step 1: Fix Test Theater
```typescript
// Before (WRONG):
it("has dashboard translation", () => {
  expect(i18n.t("nav.dashboard")).toBe("Dashboard");
});

// After (CORRECT):
it("has dashboard translation", () => {
  expect(i18n.language).toBe("nl"); // Verify behavior FIRST
  expect(i18n.t("nav.dashboard")).toBe("Dashboard"); // Then consequence
});
````

Step 2: Add Error Cases

```typescript
it("throws error if translation key missing", () => {
  expect(() => i18n.t("nonexistent.key")).toThrow();
});
```

Step 3: Add Edge Cases

```typescript
it("ignores browser language preference", () => {
  // Setup browser language = "fr"
  expect(i18n.language).toBe("nl"); // Should still be Dutch
});
```

Verdict: Cannot commit - Return to test specification ‚ùå

```

---

## Integration with /tdd-feature Command

This agent is called automatically during RED phase:

```

1. Sonnet: backend-architect ‚Üí Test specification
2. Haiku: wasp-test-automator ‚Üí Generate tests
3. üëâ Sonnet: test-quality-auditor ‚Üí Verify quality (THIS AGENT)
   IF FAIL: Return to step 1 with issues
   IF PASS: Proceed
4. Commit tests

```

---

## Success Criteria

Audit PASSES when:

1. ‚úÖ All tests FAIL (RED phase requirement)
2. ‚úÖ Failure reasons are correct (not implemented yet, NOT mock errors)
3. ‚úÖ No test theater detected (behavior verified, not just side effects)
4. ‚úÖ All mocks are actually used (if present)
5. ‚úÖ Test pattern matches implementation
6. ‚úÖ All 5 TDD criteria met for ALL tests
7. ‚úÖ Auth checks present (401 tests for operations)
8. ‚úÖ Error paths covered (400, 404, 403 as applicable)
9. ‚úÖ Edge cases included (empty, null, boundaries)

---

## Related Documentation

- **Analysis:** docs/TDD-TEST-QUALITY-ANALYSIS.md (detailed problem analysis)
- **Workflow:** docs/TDD-WORKFLOW.md (TDD methodology)
- **Command:** .claude/commands/tdd-feature.md (uses this agent)
- **Examples:** Sprint 2 commits 7f274ec, 493ac07 (real issues caught)

---

## Notes

- This agent uses **Sonnet** for optimal accuracy and speed in critical quality gate
- Runs automatically in /tdd-feature RED phase
- Output is detailed to enable precise fixes
- Prevents issues that would only surface during GREEN phase
- Cost: ~$0.009 per feature audit (3K tokens Sonnet)
- ROI: Prevents 30+ min debugging per issue caught, 80% cost reduction vs Opus
- Performance: Sonnet outperforms Opus on coding benchmarks (Sonnet 4.5: 77.2% vs Opus: 74.5% SWE-bench)

**Philosophy:** Catch test quality issues in RED phase, not GREEN phase. Tests should be "locked" before implementation begins.
```
