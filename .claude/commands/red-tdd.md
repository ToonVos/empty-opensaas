---
description: RED phase - Write comprehensive, high-quality tests with automatic artifact storage in sprintdag directory. For large features requiring extensive test planning.
---

# RED Phase: Test Specification & Generation

Write comprehensive, high-quality tests (immutable RED phase) with automatic artifact storage in your sprintdag directory structure.

## Usage

```bash
# From sprintdag directory
cd tasks/sprints/sprint-3/day-02/
/red-tdd "Add priority filtering to tasks"

# From project root (will ask for confirmation)
/red-tdd "Add priority filtering to tasks"

# With document reference
/red-tdd tasks/sprints/sprint-3/day-02/README.md
```

## Directory Detection & Artifact Storage

This command automatically detects your sprintdag directory and organizes artifacts:

```
ğŸ“ Current Directory Detection:
   â†’ Check path for: tasks/sprints/sprint-X/day-Y/
   â†’ If found: Use as artifact root
   â†’ If not found: Fallback to project root + ask confirmation

ğŸ“ Artifact Structure Created:
   tasks/sprints/sprint-3/day-02/
   â”œâ”€â”€ README.md (feature doelstelling - must exist!)
   â””â”€â”€ tests/                          # Created by /red-tdd
       â”œâ”€â”€ test-plan.md                # Test strategy document
       â”œâ”€â”€ test-suite-map.md           # Test unit boundaries
       â”œâ”€â”€ coverage-targets.json       # Expected coverage metrics
       â””â”€â”€ test-strategy-*.md          # Component-specific strategies
```

**Benefits:**

- âœ… All test artifacts stay with sprintdag context
- âœ… No cleanup needed (archived with sprint)
- âœ… Traceable: Review test decisions from specific day
- âœ… Cross-phase: /green-tdd reads from tests/ directory

## Workflow

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ PHASE 1: RED - Write Failing Tests                     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

0. ğŸ†• DIRECTORY DETECTION & VALIDATION
   â†’ Detect current directory path
   â†’ Check if in sprintdag directory (tasks/sprints/sprint-X/day-Y/)
   â†’ Validate README.md exists (feature doelstelling)
   â†’ Create tests/ subdirectory if not exists
   â†’ Store artifact root path for later use

0.5. ğŸ†• START WATCH MODE (MANDATORY)
   â†’ Launch watch mode: ./scripts/test-watch.sh
   â†’ Keep terminal open throughout RED phase
   â†’ Real-time feedback catches infrastructure issues
   â†’ Output: "Waiting for file changes..."

1. ğŸ” EXPLORE PHASE (MANDATORY - Before Analysis)
   â†’ Use Task tool with subagent_type='Explore' and thoroughness='very thorough'
   â†’ Gather comprehensive feature context:
     * Find similar features/operations (Grep for related patterns)
     * Analyze relevant entities (Read schema.prisma for related models)
     * Review related test files (Glob for similar tests)
     * Check permission patterns needed (Read permission helpers)
     * Examine error handling requirements (Read error-handling patterns)
     * Verify import rules (Read CLAUDE.md import section)
     * Analyze component types (Dialog, Sheet â†’ 3-layer strategy)
   â†’ Output: Feature context document with patterns, constraints, examples
   â†’ **Why critical:** Prevents implementing features that ignore existing patterns

2. ğŸ“‹ PLAN PHASE (MANDATORY - Before Test Generation)
   â†’ Use Task tool with subagent_type='Plan' and model='sonnet'
   â†’ Create comprehensive test generation strategy:
     * Plan test scenarios (auth, validation, edge cases, success)
     * Design mock strategy (which entities to mock vs real DB)
     * **CRITICAL:** Use `vi.mock()` ONLY (NEVER suggest MSW installation - breaks project consistency)
     * Determine test pattern (unit vs integration)
     * Assess component type (standard vs portal components)
     * Sequence test cases (order matters for readability)
     * Define assertion strategy (specific, not generic)
     * Plan test data setup (fixtures vs inline vs factories)
     * Identify test helper opportunities (reusable mocks, setups)
     * Estimate coverage targets (â‰¥80% statements, â‰¥75% branches)
   â†’ Output: Test generation plan with scenarios and patterns
   â†’ **Why critical:** Ensures comprehensive test coverage before writing code

3. ARCHITECTURE ANALYSIS
   â†’ Invoke: Task tool with subagent_type='backend-architect' and model='sonnet'
   â†’ Review exploration findings and test plan
   â†’ Analyze requirements
   â†’ Design API/component structure
   â†’ Identify all test scenarios (auth, validation, edge cases, success)
   â†’ Specify test pattern (unit vs integration)
   â†’ Determine component testing strategy:
     * Standard components: userEvent + waitFor pattern
     * Portal components (Dialog, Sheet): 3-layer test strategy
   â†’ Output: Detailed test specification

4. TEST GENERATION
   â†’ Invoke: Task tool with subagent_type='wasp-test-automator' and model='haiku'
   â†’ Generate complete test files from specification
   â†’ Follow test pattern guidance (unit vs integration)
   â†’ Apply component-specific strategies (3-layer for portals)
   â†’ Include behavior verification (not just side effects)
   â†’ Apply Wasp import rules (wasp/, @prisma/client)
   â†’ Self-validation:
     * Auto-fix path aliases (s/lib/utils â†’ ../../lib/utils)
     * Hoist enum imports (@prisma/client for runtime values)
     * Verify all imports resolve
   â†’ Watch terminal auto-runs tests on save
   â†’ Output: Test files in app/src/**/*.test.ts

5. VERIFY EXECUTION IN WATCH MODE
   â†’ Check watch terminal output
   â†’ Expected: Tests EXECUTE (no timeouts)
   â†’ Expected: Tests FAIL with clear error (e.g., "Cannot find module")
   â†’ NOT expected: "Test timed out in 5000ms"
   â†’ If timeouts â†’ Fix infrastructure â†’ Watch reruns
   â†’ Output: Confirmation tests execute properly

6. TEST QUALITY AUDIT
   â†’ Invoke: Task tool with subagent_type='test-quality-auditor' and model='sonnet'
   â†’ Pre-audit: Verify no timeouts, no import errors
   â†’ Run tests, analyze failure reasons
   â†’ Check for test theater patterns (side effects vs behavior)
   â†’ Verify mocks actually used (if present)
   â†’ Match test pattern to implementation (unit vs integration)
   â†’ Validate 5 TDD criteria with code analysis:
     1. Tests business logic (NOT existence checks)
     2. Meaningful assertions (NOT just .toBeDefined())
     3. Tests error paths (401, 403, 404, 400)
     4. Tests edge cases (empty, null, boundaries)
     5. Tests behavior (NOT implementation details)
   â†’ Output: Pass/Fail + detailed issues

   IF audit FAILS:
     â†’ Return to step 2 (PLAN) with issues
     â†’ Revise test specification
     â†’ Regenerate tests (step 4)
     â†’ Re-audit (step 6)

7. RUN TESTS & VERIFY FAILURES
   â†’ Execute: cd app && wasp test client run
   â†’ Verify ALL tests FAIL for correct reason:
     âœ… "Cannot find module" (operation doesn't exist yet)
     âœ… "Property 'X' does not exist" (entity field missing)
     âŒ "Test timed out" (infrastructure issue - FIX!)
     âŒ "Cannot find module 'wasp/...'" (import error - FIX!)
   â†’ Output: All tests RED (as expected)

8. WRITE ARTIFACTS TO tests/ DIRECTORY
   â†’ Create artifact directory: [dag-directory]/tests/
   â†’ Write test-plan.md:
     * Test scenarios (all cases identified)
     * Mock strategy (what to mock, why)
     * Test pattern (unit vs integration justification)
     * Assertion strategy (what to verify)
     * Test data approach (fixtures/inline/factories)
     * Coverage targets (specific percentages)
   â†’ Write test-suite-map.md (if multi-file):
     * Test file organization
     * Test unit boundaries (what each file tests)
     * Test dependencies (which tests depend on which schema state)
   â†’ Write coverage-targets.json:
     * Expected coverage metrics (statements, branches, functions, lines)
     * Per-file targets (if different from global)
   â†’ Write test-strategy-[component].md (if component tests):
     * Component type (standard vs portal)
     * Testing strategy (userEvent pattern vs 3-layer)
     * Special considerations (polyfills, async waits)
   â†’ Output: Artifacts written to tests/ subdirectory

9. GIT COMMIT (IMMUTABLE)
   â†’ Stage test files only: git add app/src/**/*.test.ts
   â†’ Commit with message: "test: Add [feature] tests (RED)"
   â†’ Output: Tests committed (now immutable)

10. SUMMARY & NEXT STEPS
    â†’ Display summary:
      âœ… Tests written: X test files, Y test cases
      âœ… Test quality: 5 TDD criteria PASS
      âœ… Tests committed: [commit hash]
      âœ… Artifacts: [dag-directory]/tests/
      âœ… Coverage targets: [percentage]
    â†’ Next step: Run /green-tdd "[feature-name]" to implement
```

## Prerequisites

Before running `/red-tdd`:

1. âœ… **Sprintdag directory exists** (tasks/sprints/sprint-X/day-Y/)
2. âœ… **README.md exists** with feature doelstelling
3. âœ… **Watch mode available** (./scripts/test-watch.sh works)
4. âœ… **No uncommitted test changes** (git status clean for tests/)

## Exit Criteria

This command completes successfully when:

1. âœ… Tests execute without syntax errors
2. âœ… All tests FAIL for correct reason (not timeout/import errors)
3. âœ… 5 TDD criteria PASS (verified by test-quality-auditor)
4. âœ… Tests committed to git (immutable)
5. âœ… Artifacts written to tests/ directory

## Artifacts Created

After successful completion, you'll find:

```
tasks/sprints/sprint-3/day-02/tests/
â”œâ”€â”€ test-plan.md              # Test strategy document
â”œâ”€â”€ test-suite-map.md         # Test organization (if multi-file)
â”œâ”€â”€ coverage-targets.json     # Expected coverage
â””â”€â”€ test-strategy-*.md        # Component-specific strategies

app/src/**/*.test.ts          # Committed test files (immutable)
```

## Agent Assignment

| Step | Task                  | Model  | Agent                  | Reason                        |
| ---- | --------------------- | ------ | ---------------------- | ----------------------------- |
| 1    | Context exploration   | Haiku  | **Explore** (built-in) | Fast codebase search          |
| 2    | Test planning         | Sonnet | **Plan** (built-in)    | Strategic test design         |
| 3    | Requirements analysis | Sonnet | backend-architect      | Complex reasoning             |
| 4    | Test generation       | Haiku  | wasp-test-automator    | Pattern-based generation      |
| 6    | Test quality audit    | Opus   | test-quality-auditor   | Critical quality verification |

## New Capabilities for Large Features

**vs Unified /tdd-feature:**

1. **Test Decomposition** - Split large features into testable units

   - Analyze feature boundaries
   - Create test-suite-map.md with unit organization
   - Enable parallel test writing (multiple /red-tdd sessions)

2. **Test Helper Extraction** - Build reusable test utilities

   - Identify duplication patterns in mocks/fixtures
   - Extract to app/src/test/helpers/\*.ts
   - Document in test-plan.md

3. **Component Type Assessment** - Earlier portal detection

   - Analyze component imports (Dialog, Sheet, AlertDialog)
   - Choose 3-layer vs standard strategy
   - Document in test-strategy-[component].md

4. **Test Data Factory** - Generate realistic test data

   - Plan fixture strategy (inline vs factory)
   - Generate factories in app/src/test/factories/\*.ts
   - Reuse across test files

5. **Artifact Persistence** - State preserved for /green-tdd
   - Test plan available for implementation phase
   - Coverage targets tracked across phases
   - Decisions documented and traceable

## Cross-Phase Integration

**This command prepares for /green-tdd:**

- âœ… **test-plan.md** â†’ /green-tdd reads test scenarios for implementation guidance
- âœ… **coverage-targets.json** â†’ /green-tdd validates actual coverage meets targets
- âœ… **test-suite-map.md** â†’ /green-tdd knows which units to implement
- âœ… **Committed tests** â†’ /green-tdd prerequisite validation (tests must exist & be committed)

## Example Execution

**Command:**

```bash
cd tasks/sprints/sprint-3/day-02/
/red-tdd "Add priority filtering to tasks"
```

**Output:**

```
ğŸ“ Directory Detection...
   âœ“ Sprintdag directory: tasks/sprints/sprint-3/day-02/
   âœ“ README.md exists: "Implement priority filtering"
   âœ“ Creating tests/ subdirectory

ğŸ” Starting watch mode...
   âœ“ ./scripts/test-watch.sh running
   âœ“ "Waiting for file changes..."

ğŸ” EXPLORE: Gathering feature context...
   Model: Haiku (Explore agent)
   âœ“ Found similar feature: status filtering (src/server/a3/operations.ts:123)
   âœ“ Relevant entities: Task, Priority enum (schema.prisma:45)
   âœ“ Permission pattern: requireDepartmentAccess (src/permissions/helpers.ts:12)
   âœ“ Test examples: src/server/a3/operations.test.ts

ğŸ“‹ PLAN: Creating test strategy...
   Model: Sonnet (Plan agent)
   Test scenarios identified:
   âœ“ Auth: 401 not authenticated
   âœ“ Auth: 403 no department access
   âœ“ Validation: 400 invalid priority enum
   âœ“ Success: Filter by single priority
   âœ“ Success: Filter by multiple priorities
   âœ“ Edge: No tasks matching filter
   âœ“ Edge: Null/undefined priority
   Mock strategy: Integration tests (real DB)
   Coverage target: â‰¥80% statements, â‰¥75% branches

ğŸ¯ ARCHITECTURE: Analyzing requirements...
   Model: Sonnet (backend-architect)
   âœ“ API structure: getTasks(filters: { priority?: Priority[] })
   âœ“ Test pattern: Integration (Prisma queries)
   âœ“ Test specification: 7 test cases

âš¡ TEST GENERATION: Creating test files...
   Model: Haiku (wasp-test-automator)
   âœ“ Created: app/src/server/a3/operations.test.ts
   âœ“ Self-validation: All imports correct
   âœ“ Watch mode: Tests executed (342ms)

ğŸ” QUALITY AUDIT: Verifying test quality...
   Model: Sonnet (test-quality-auditor)
   Pre-audit:
   âœ“ No timeouts (all <1000ms)
   âœ“ No import errors
   Test quality:
   âœ“ Criterion 1: Tests business logic (filter behavior)
   âœ“ Criterion 2: Meaningful assertions (expect exact matches)
   âœ“ Criterion 3: Error paths tested (401, 403, 400)
   âœ“ Criterion 4: Edge cases covered (empty, null)
   âœ“ Criterion 5: Tests behavior (observable results)
   Audit: PASS âœ…

âœ… TESTS VERIFIED: All RED for correct reasons
   7/7 tests FAIL: "Cannot find module 'operations'" (expected)

ğŸ“ ARTIFACTS: Writing to tests/ directory...
   âœ“ tests/test-plan.md (test strategy)
   âœ“ tests/coverage-targets.json (80%/75% targets)

ğŸ“ GIT COMMIT: Committing tests...
   âœ“ Staged: app/src/server/a3/operations.test.ts
   âœ“ Committed: test: Add priority filtering tests (RED)
   âœ“ Commit hash: a1b2c3d

ğŸ‰ RED PHASE COMPLETE!

   Summary:
   âœ… Tests written: 1 file, 7 test cases
   âœ… Test quality: 5 TDD criteria PASS
   âœ… Tests committed: a1b2c3d
   âœ… Artifacts: tasks/sprints/sprint-3/day-02/tests/
   âœ… Coverage targets: 80% statements, 75% branches

   Next step:
   â†’ Run: /green-tdd "priority-filtering"
```

## Error Handling

**If directory detection fails:**

```
âŒ Not in sprintdag directory
   Current: /Users/user/Projects/lean-ai-coach-Dev3/
   Expected: tasks/sprints/sprint-X/day-Y/

   Options:
   1. cd to sprintdag directory and retry
   2. Continue with project root (will ask confirmation)
```

**If README.md missing:**

```
âŒ Feature doelstelling not found
   Expected: README.md in current directory

   Action: Create README.md with feature description
```

**If test quality audit fails:**

```
âŒ Test quality audit FAILED

   Issues found:
   - Test theater: Tests check side effects, not behavior
   - Missing auth: No 401/403 tests
   - Generic assertions: Using .toBeDefined()
   - No edge cases: Missing empty/null tests

   Action:
   â†’ Returning to PLAN phase with issues
   â†’ Will revise test specification
   â†’ Will regenerate tests
```

**If watch mode has timeouts:**

```
âŒ Tests timing out in watch mode

   Diagnosis:
   - Portal component without 3-layer strategy?
   - Missing await for async operations?
   - Polyfills not loaded?

   Action:
   â†’ Fix infrastructure issues
   â†’ Regenerate tests with correct strategy
   â†’ Watch mode will auto-rerun
```

## When to Use This Command

âœ… **Use /red-tdd for:**

- Large features (>5 operations, >500 LOC)
- Complex features (multi-entity, complex business logic)
- Features requiring extensive test planning
- Features split across multiple test files
- Features with portal components (Dialog, Sheet)
- Multi-worktree features (parallel test writing)

âŒ **Use /tdd-feature instead for:**

- Small features (<5 operations, <500 LOC)
- Simple CRUD operations
- Features with straightforward test scenarios
- Quick prototyping (want speed, not rigor)

## Integration with Other Commands

**Followed by:**

- `/green-tdd "[feature-name]"` - Implement code to pass tests
- `/refactor-tdd "[feature-name]"` - Improve code quality
- `/security-tdd "[feature-name]"` - Security audit

**Reads artifacts from:**

- `README.md` (current directory) - Feature doelstelling

**Creates artifacts for:**

- `/green-tdd` - test-plan.md, coverage-targets.json
- `/refactor-tdd` - coverage-targets.json (baseline)
- `/security-tdd` - test-plan.md (expected security scenarios)

## References

- **Agents:** wasp-test-automator, test-quality-auditor, backend-architect
- **Skills:** /tdd-workflow (5 TDD criteria), /wasp-operations (test patterns)
- **Docs:** docs/TDD-WORKFLOW.md, docs/TDD-TEST-QUALITY-ANALYSIS.md
- **Templates:** .claude/templates/test.template.ts
- **Marketplace:** backend-development (Sonnet architect)
